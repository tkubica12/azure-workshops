# Sentiment Analysis Approach Comparison

A comprehensive experimental comparison of different approaches to sentiment analysis, evaluating Large Language Models (LLMs), fine-tuned models, embedding-based methods, and transformer architectures across accuracy, cost, and performance metrics.

## ğŸ¯ Purpose

This project systematically compares 14 different sentiment analysis approaches to help practitioners choose the optimal method based on their specific requirements:

- **Accuracy requirements** (60.9% - 80.0% range observed)
- **Cost constraints** ($0.0005 - $10.67 per 1K samples)
- **Processing speed needs** (0.09 - 3.00 seconds per sample)
- **Infrastructure preferences** (cloud APIs vs local deployment)
- **Data privacy requirements**

## ğŸ§ª Approaches Tested

### 1. Large Language Model (LLM) Approaches
- **GPT-4.1-nano**: Zero-shot, few-shot (100/1000 examples)
- **GPT-4.1-mini**: Zero-shot, few-shot (100/1000 examples)

### 2. Fine-tuned LLM Approaches  
- **GPT-4.1-nano Fine-tuned**: Zero-shot, few-shot (100 examples)
- **GPT-4.1-mini Fine-tuned**: Zero-shot, few-shot (100 examples)

### 3. Embedding + Machine Learning Approaches
- **OpenAI Embeddings + Logistic Regression**: Trained on 100, 1000, and full dataset

### 4. Fine-tuned Transformer Approaches
- **BERT-base Fine-tuned**: Traditional encoder-only transformer model

## ğŸ“ Project Structure

```
d-ai-sentiment/
â”œâ”€â”€ README.md                     # This file
â”œâ”€â”€ config.yaml                   # Configuration settings
â”œâ”€â”€ pyproject.toml               # Python dependencies
â”œâ”€â”€ 
â”œâ”€â”€ dataset/                     # Dataset files
â”‚   â”œâ”€â”€ train_df.csv            # Training data
â”‚   â”œâ”€â”€ val_df.csv              # Validation data
â”‚   â”œâ”€â”€ test_df.csv             # Test data
â”‚   â””â”€â”€ README.md               # Dataset documentation
â”œâ”€â”€ 
â”œâ”€â”€ results/                     # Experiment results and analysis
â”‚   â”œâ”€â”€ README.md               # ğŸ“Š COMPREHENSIVE RESULTS ANALYSIS
â”‚   â”œâ”€â”€ *_results_*.csv         # Individual experiment results
â”‚   â”œâ”€â”€ *_results_*.txt         # Detailed experiment reports
â”‚   â””â”€â”€ *_training_*.txt        # Training logs and statistics
â”œâ”€â”€ 
â”œâ”€â”€ models/                      # Trained model artifacts
â”œâ”€â”€ train/                       # Training utilities
â”œâ”€â”€ utils/                       # Helper functions
â”œâ”€â”€ 
â”œâ”€â”€ LLM experiment scripts:
â”œâ”€â”€ llm_nano_zeroshot.py        # GPT-4.1-nano zero-shot
â”œâ”€â”€ llm_nano_fewshot_100.py     # GPT-4.1-nano few-shot (100)
â”œâ”€â”€ llm_nano_fewshot_1000.py    # GPT-4.1-nano few-shot (1000)
â”œâ”€â”€ llm_mini_zeroshot.py        # GPT-4.1-mini zero-shot
â”œâ”€â”€ llm_mini_fewshot_100.py     # GPT-4.1-mini few-shot (100)
â”œâ”€â”€ llm_mini_fewshot_1000.py    # GPT-4.1-mini few-shot (1000)
â”œâ”€â”€ llm_ft_nano_zeroshot.py     # Fine-tuned nano zero-shot
â”œâ”€â”€ llm_ft_nano_fewshot_100.py  # Fine-tuned nano few-shot (100)
â”œâ”€â”€ llm_ft_mini_zeroshot.py     # Fine-tuned mini zero-shot
â”œâ”€â”€ llm_ft_mini_fewshot_100.py  # Fine-tuned mini few-shot (100)
â”œâ”€â”€ 
â”œâ”€â”€ ML experiment scripts:
â”œâ”€â”€ train_lr_100.py             # Train LR with 100 samples
â”œâ”€â”€ train_lr_1000.py            # Train LR with 1000 samples  
â”œâ”€â”€ train_lr_all.py             # Train LR with full dataset
â”œâ”€â”€ test_lr_100.py              # Test LR 100 model
â”œâ”€â”€ test_lr_1000.py             # Test LR 1000 model
â”œâ”€â”€ test_lr_all.py              # Test LR full model
â”œâ”€â”€ 
â”œâ”€â”€ BERT experiment scripts:
â”œâ”€â”€ train_bert.py               # Train BERT model
â”œâ”€â”€ train_bert_full.py          # Full BERT training pipeline
â”œâ”€â”€ test_bert.py                # Test BERT model
â””â”€â”€ 
â””â”€â”€ Data preparation:
    â”œâ”€â”€ prepare_training_files.py   # Prepare fine-tuning data
    â”œâ”€â”€ prepare_training_embeddings.py # Generate embeddings
    â”œâ”€â”€ dataset_stats.py           # Dataset analysis
    â””â”€â”€ dataset_trim.py            # Dataset trimming utilities
```

## ğŸ† Key Results Summary

| Approach Category | Best Method | Accuracy | Cost per 1K | Speed | Key Advantage |
|------------------|-------------|----------|-------------|-------|---------------|
| **Overall Winner** | LR_ALL | 73.2% | $0.0005 | 5.58/sec | Best cost-effectiveness |
| **Highest Accuracy** | LLM_FT_MINI_ZEROSHOT | 80.0% | $0.202 | 3.30/sec | Maximum accuracy |
| **Fastest Processing** | BERT_SENTIMENT | 74.5% | $0.028 | 11.19/sec | Local inference speed |
| **Best Balance** | BERT_SENTIMENT | 74.5% | $0.028 | 11.19/sec | Accuracy + speed + privacy |

### Cost-Effectiveness Ranking
1. **Embedding+ML approaches**: $0.0005 per 1K samples (50-21,000x more cost-effective)
2. **LLM base models**: $0.025-$0.106 per 1K samples  
3. **BERT**: $0.028 per 1K samples (excellent accuracy-to-cost ratio)
4. **Fine-tuned LLMs**: $0.050-$0.202 per 1K samples (highest accuracy)

## ğŸ¯ Quick Recommendations

### Choose Based on Your Priority:

- **ğŸ’° Cost-Sensitive**: Use **LR_ALL** ($0.0005/1K, 73.2% accuracy)
- **ğŸ¯ Maximum Accuracy**: Use **LLM_FT_MINI_ZEROSHOT** ($0.202/1K, 80.0% accuracy)  
- **âš¡ Speed + Accuracy**: Use **BERT_SENTIMENT** ($0.028/1K, 74.5% accuracy, 11.19/sec)
- **ğŸ”’ Data Privacy**: Use **BERT_SENTIMENT** (local deployment, no external APIs)
- **ğŸš€ Rapid Prototyping**: Use **LLM_NANO_ZEROSHOT** (no training required, 67.7% accuracy)

## ğŸ“Š Detailed Analysis

For comprehensive results including:
- Detailed performance metrics by sentiment class
- Cost analysis with caching considerations  
- Processing speed comparisons
- Technical methodology explanations
- Production deployment recommendations
- Break-even analysis for different approaches

**â¡ï¸ See the complete analysis: [results/README.md](results/README.md)**

## ğŸ“ˆ Dataset

Multi-class sentiment analysis dataset with:
- **Training**: 24,985 samples
- **Validation**: 6,247 samples  
- **Test**: 5,205 samples
- **Classes**: Negative (0), Neutral (1), Positive (2)

See [dataset/README.md](dataset/README.md) for detailed dataset information.