{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG demo level 2\n",
    "In more advanced demonstration we will add hierarchical and graph approaches by extracting metadata, finding and storing relationships between documents and adding summarizations for aggregate questions.\n",
    "\n",
    "## Step 1 - Extracting metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data CSV data to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>overview</th>\n",
       "      <th>release_date</th>\n",
       "      <th>popularity</th>\n",
       "      <th>vote_average</th>\n",
       "      <th>vote_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>19404</td>\n",
       "      <td>Dilwale Dulhania Le Jayenge</td>\n",
       "      <td>Raj is a rich, carefree, happy-go-lucky second...</td>\n",
       "      <td>1995-10-20</td>\n",
       "      <td>18.433</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>724089</td>\n",
       "      <td>Gabriel's Inferno Part II</td>\n",
       "      <td>Professor Gabriel Emerson finally learns the t...</td>\n",
       "      <td>2020-07-31</td>\n",
       "      <td>8.439</td>\n",
       "      <td>8.7</td>\n",
       "      <td>1223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>278</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>Framed in the 1940s for the double murder of h...</td>\n",
       "      <td>1994-09-23</td>\n",
       "      <td>65.570</td>\n",
       "      <td>8.7</td>\n",
       "      <td>18637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>238</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>Spanning the years 1945 to 1955, a chronicle o...</td>\n",
       "      <td>1972-03-14</td>\n",
       "      <td>63.277</td>\n",
       "      <td>8.7</td>\n",
       "      <td>14052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>761053</td>\n",
       "      <td>Gabriel's Inferno Part III</td>\n",
       "      <td>The final part of the film adaption of the ero...</td>\n",
       "      <td>2020-11-19</td>\n",
       "      <td>26.691</td>\n",
       "      <td>8.7</td>\n",
       "      <td>773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id                        title  \\\n",
       "0           0   19404  Dilwale Dulhania Le Jayenge   \n",
       "1           1  724089    Gabriel's Inferno Part II   \n",
       "2           2     278     The Shawshank Redemption   \n",
       "3           3     238                The Godfather   \n",
       "4           4  761053   Gabriel's Inferno Part III   \n",
       "\n",
       "                                            overview release_date  popularity  \\\n",
       "0  Raj is a rich, carefree, happy-go-lucky second...   1995-10-20      18.433   \n",
       "1  Professor Gabriel Emerson finally learns the t...   2020-07-31       8.439   \n",
       "2  Framed in the 1940s for the double murder of h...   1994-09-23      65.570   \n",
       "3  Spanning the years 1945 to 1955, a chronicle o...   1972-03-14      63.277   \n",
       "4  The final part of the film adaption of the ero...   2020-11-19      26.691   \n",
       "\n",
       "   vote_average  vote_count  \n",
       "0           8.7        2763  \n",
       "1           8.7        1223  \n",
       "2           8.7       18637  \n",
       "3           8.7       14052  \n",
       "4           8.7         773  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/movies.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure to deploy Azure infrastructure using ```terraform apply``` command in terraform folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using postgresql://:)ycxlsxlLRKks*g#@/?sslmode=require as the database connection string\n",
      "Using https://graphrag-psbv.openai.azure.com/ as the embedding model endpoint\n",
      "Using https://graphrag-psbv.openai.azure.com/ as the gpt-4o-mini model endpoint\n",
      "Using https://graphrag-psbv.openai.azure.com/ as the gpt-4o model endpoint\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "try:\n",
    "    # Jump into the terraform directory\n",
    "    os.chdir('terraform')\n",
    "\n",
    "    # Get the database connection string\n",
    "    PGHOST = subprocess.run(['terraform', 'output', '-raw', 'PGHOST'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    PGDATABASE = subprocess.run(['terraform', 'output', '-raw', 'PGDATABASE'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    PGUSER = subprocess.run(['terraform', 'output', '-raw', 'PGUSER'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    PGPASSWORD = subprocess.run(['terraform', 'output', '-raw', 'PGPASSWORD'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    db_uri = f\"postgresql://{PGUSER}:{PGPASSWORD}@{PGHOST}/{PGDATABASE}?sslmode=require\"\n",
    "\n",
    "    # Get the embedding model endpoint and key\n",
    "    model_configurations = subprocess.run(['terraform', 'output', '-raw', 'model_configurations'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    model_config = json.loads(model_configurations)\n",
    "    embedding_model = model_config[\"models\"][\"text-embedding-3-large\"]\n",
    "    EMBEDDINGS_ENDPOINT = embedding_model[\"endpoint\"]\n",
    "    EMBEDDINGS_KEY = embedding_model[\"key\"]\n",
    "    gpt_4o_mini_model = model_config[\"models\"][\"gpt-4o-mini\"]\n",
    "    GPT_4O_MINI_ENDPOINT = gpt_4o_mini_model[\"endpoint\"]\n",
    "    GPT_4O_MINI_KEY = gpt_4o_mini_model[\"key\"]\n",
    "    gpt_4o_model = model_config[\"models\"][\"gpt-4o\"]\n",
    "    GPT_4O_ENDPOINT = gpt_4o_model[\"endpoint\"]\n",
    "    GPT_4O_KEY = gpt_4o_model[\"key\"]\n",
    "\n",
    "    print(f\"Using {db_uri} as the database connection string\")\n",
    "    print(f\"Using {EMBEDDINGS_ENDPOINT} as the embedding model endpoint\")\n",
    "    print(f\"Using {GPT_4O_MINI_ENDPOINT} as the gpt-4o-mini model endpoint\")\n",
    "    print(f\"Using {GPT_4O_ENDPOINT} as the gpt-4o model endpoint\")\n",
    "\n",
    "finally:\n",
    "    os.chdir(original_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup LLM connection and define extraction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI \n",
    "\n",
    "gpt_4o_client = AzureOpenAI(  \n",
    "    azure_endpoint=GPT_4O_ENDPOINT,  \n",
    "    api_key=GPT_4O_KEY,  \n",
    "    api_version=\"2024-08-01-preview\",\n",
    ")\n",
    "\n",
    "gpt_4o_mini_client = AzureOpenAI(\n",
    "    azure_endpoint=GPT_4O_MINI_ENDPOINT,  \n",
    "    api_key=GPT_4O_MINI_KEY,  \n",
    "    api_version=\"2024-08-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jinja2\n",
    "from MoviesDataClasses import EnhancedMovie\n",
    "from pydantic import BaseModel, ValidationError\n",
    "\n",
    "template_loader = jinja2.FileSystemLoader(searchpath=\"./prompts\")\n",
    "template_env = jinja2.Environment(loader=template_loader)\n",
    "template_extract = template_env.get_template(\"extract_from_movie.jinja2\")\n",
    "\n",
    "def extract(title: str, description: str):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": template_extract.render(title=title, description=description)}\n",
    "    ]\n",
    "    try:\n",
    "        completion = gpt_4o_client.beta.chat.completions.parse(  \n",
    "            model=\"gpt-4o\",\n",
    "            messages=messages,\n",
    "            max_tokens=200,  \n",
    "            temperature=0.7,\n",
    "            response_format=EnhancedMovie\n",
    "        )\n",
    "        response_content = completion.choices[0].message.parsed\n",
    "        return response_content\n",
    "    except Exception as e:\n",
    "        return EnhancedMovie(\n",
    "            genres=[],\n",
    "            characters=[],\n",
    "            themes=[],\n",
    "            setting=[],\n",
    "            series=[],\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract metadata from movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "start_index = 0\n",
    "batch_size = 200\n",
    "results = []\n",
    "batch_index = start_index // batch_size\n",
    "\n",
    "# Process rows starting from start_index\n",
    "for idx in range(start_index, len(df)):\n",
    "    row = df.iloc[idx]\n",
    "    try:\n",
    "        print(f\"{idx}: Extracting {row['title']}...\")\n",
    "        movie = extract(row[\"title\"], row[\"overview\"])\n",
    "        enhanced_row = row.to_dict()\n",
    "        enhanced_row.update(movie.model_dump())\n",
    "        results.append(enhanced_row)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {idx}: {e}\")\n",
    "    \n",
    "    # Export current batch every batch_size rows\n",
    "    if ((idx + 1 - start_index) % batch_size) == 0:\n",
    "        batch_filename = f\"data/movies_graph_batch_{batch_index}.json\"\n",
    "        pd.DataFrame(results).to_json(batch_filename, orient=\"records\", indent=2)\n",
    "        print(f\">>> Exported batch {batch_index} up to row {idx}\")\n",
    "        batch_index += 1\n",
    "        results = []\n",
    "\n",
    "# Export any remaining movies if they exist\n",
    "if results:\n",
    "    batch_filename = f\"data/movies_graph_batch_{batch_index}.json\"\n",
    "    pd.DataFrame(results).to_json(batch_filename, orient=\"records\", indent=2)\n",
    "    print(f\">>> Exported final batch {batch_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged all batches into data/movies_graph.json\n",
      "Deleted data\\movies_graph_batch_0.json\n",
      "Deleted data\\movies_graph_batch_1.json\n",
      "Deleted data\\movies_graph_batch_2.json\n",
      "Deleted data\\movies_graph_batch_3.json\n",
      "Deleted data\\movies_graph_batch_4.json\n",
      "Deleted data\\movies_graph_batch_5.json\n",
      "Deleted data\\movies_graph_batch_6.json\n",
      "Deleted data\\movies_graph_batch_7.json\n",
      "Deleted data\\movies_graph_batch_8.json\n",
      "Deleted data\\movies_graph_batch_9.json\n",
      "Deleted data\\movies_graph_batch_10.json\n",
      "Deleted data\\movies_graph_batch_11.json\n",
      "Deleted data\\movies_graph_batch_12.json\n",
      "Deleted data\\movies_graph_batch_13.json\n",
      "Deleted data\\movies_graph_batch_14.json\n",
      "Deleted data\\movies_graph_batch_15.json\n",
      "Deleted data\\movies_graph_batch_16.json\n",
      "Deleted data\\movies_graph_batch_17.json\n",
      "Deleted data\\movies_graph_batch_18.json\n",
      "Deleted data\\movies_graph_batch_19.json\n",
      "Deleted data\\movies_graph_batch_20.json\n",
      "Deleted data\\movies_graph_batch_21.json\n",
      "Deleted data\\movies_graph_batch_22.json\n",
      "Deleted data\\movies_graph_batch_23.json\n",
      "Deleted data\\movies_graph_batch_24.json\n",
      "Deleted data\\movies_graph_batch_25.json\n",
      "Deleted data\\movies_graph_batch_26.json\n",
      "Deleted data\\movies_graph_batch_27.json\n",
      "Deleted data\\movies_graph_batch_28.json\n",
      "Deleted data\\movies_graph_batch_29.json\n",
      "Deleted data\\movies_graph_batch_30.json\n",
      "Deleted data\\movies_graph_batch_31.json\n",
      "Deleted data\\movies_graph_batch_32.json\n",
      "Deleted data\\movies_graph_batch_33.json\n",
      "Deleted data\\movies_graph_batch_34.json\n",
      "Deleted data\\movies_graph_batch_35.json\n",
      "Deleted data\\movies_graph_batch_36.json\n",
      "Deleted data\\movies_graph_batch_37.json\n",
      "Deleted data\\movies_graph_batch_38.json\n",
      "Deleted data\\movies_graph_batch_39.json\n",
      "Deleted data\\movies_graph_batch_40.json\n",
      "Deleted data\\movies_graph_batch_41.json\n",
      "Deleted data\\movies_graph_batch_42.json\n"
     ]
    }
   ],
   "source": [
    "# Merge all batch files into a single JSON file\n",
    "batch_files = sorted(\n",
    "    glob.glob(\"data/movies_graph_batch_*.json\"),\n",
    "    key=lambda f: int(re.search(r'(\\d+)', f).group(1))\n",
    ")\n",
    "\n",
    "all_data = []\n",
    "for file in batch_files:\n",
    "    df_batch = pd.read_json(file)\n",
    "    all_data.extend(df_batch.to_dict(orient=\"records\"))\n",
    "    \n",
    "merged_df = pd.DataFrame(all_data)\n",
    "merged_output_file = \"data/movies_graph.json\"\n",
    "merged_df.to_json(merged_output_file, orient=\"records\", indent=2)\n",
    "print(f\"Merged all batches into {merged_output_file}\")\n",
    "\n",
    "# Delete individual batch files after a successful merge\n",
    "for file in batch_files:\n",
    "    os.remove(file)\n",
    "    print(f\"Deleted {file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
