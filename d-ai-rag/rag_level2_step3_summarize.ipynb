{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG demo level 2\n",
    "In more advanced demonstration we will add hierarchical and graph approaches by extracting metadata, finding and storing relationships between documents and adding summarizations for aggregate questions.\n",
    "\n",
    "## Step 3 - Adding LLM summarizations to entities (communities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using postgresql://psqladmin:)ycxlsxlLRKks*g#@psql-graphrag-psbv.postgres.database.azure.com/demo?sslmode=require as the database connection string\n",
      "Using https://graphrag-psbv.openai.azure.com/ as the embedding model endpoint\n",
      "Using https://graphrag-psbv.openai.azure.com/ as the gpt-4o-mini model endpoint\n",
      "Using https://graphrag-psbv.openai.azure.com/ as the gpt-4o model endpoint\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "try:\n",
    "    # Jump into the terraform directory\n",
    "    os.chdir('terraform')\n",
    "\n",
    "    # Get the database connection string\n",
    "    PGHOST = subprocess.run(['terraform', 'output', '-raw', 'PGHOST'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    PGDATABASE = subprocess.run(['terraform', 'output', '-raw', 'PGDATABASE'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    PGUSER = subprocess.run(['terraform', 'output', '-raw', 'PGUSER'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    PGPASSWORD = subprocess.run(['terraform', 'output', '-raw', 'PGPASSWORD'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    db_uri = f\"postgresql://{PGUSER}:{PGPASSWORD}@{PGHOST}/{PGDATABASE}?sslmode=require\"\n",
    "\n",
    "    # Get the embedding model endpoint and key\n",
    "    model_configurations = subprocess.run(['terraform', 'output', '-raw', 'model_configurations'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    model_config = json.loads(model_configurations)\n",
    "    embedding_model = model_config[\"models\"][\"text-embedding-3-large\"]\n",
    "    EMBEDDINGS_ENDPOINT = embedding_model[\"endpoint\"]\n",
    "    EMBEDDINGS_KEY = embedding_model[\"key\"]\n",
    "    gpt_4o_mini_model = model_config[\"models\"][\"gpt-4o-mini\"]\n",
    "    GPT_4O_MINI_ENDPOINT = gpt_4o_mini_model[\"endpoint\"]\n",
    "    GPT_4O_MINI_KEY = gpt_4o_mini_model[\"key\"]\n",
    "    gpt_4o_model = model_config[\"models\"][\"gpt-4o\"]\n",
    "    GPT_4O_ENDPOINT = gpt_4o_model[\"endpoint\"]\n",
    "    GPT_4O_KEY = gpt_4o_model[\"key\"]\n",
    "\n",
    "    print(f\"Using {db_uri} as the database connection string\")\n",
    "    print(f\"Using {EMBEDDINGS_ENDPOINT} as the embedding model endpoint\")\n",
    "    print(f\"Using {GPT_4O_MINI_ENDPOINT} as the gpt-4o-mini model endpoint\")\n",
    "    print(f\"Using {GPT_4O_ENDPOINT} as the gpt-4o model endpoint\")\n",
    "\n",
    "finally:\n",
    "    os.chdir(original_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create clients for PostgreSQL and OpenAI models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from openai import AzureOpenAI \n",
    "import pandas as pd\n",
    "import age\n",
    "\n",
    "conn = psycopg2.connect(db_uri)\n",
    "\n",
    "gpt_4o_client = AzureOpenAI(  \n",
    "    azure_endpoint=GPT_4O_ENDPOINT,  \n",
    "    api_key=GPT_4O_KEY,  \n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")\n",
    "\n",
    "gpt_4o_mini_client = AzureOpenAI(\n",
    "    azure_endpoint=GPT_4O_MINI_ENDPOINT,  \n",
    "    api_key=GPT_4O_MINI_KEY,  \n",
    "    api_version=\"2024-05-01-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable AGE for this connection\n",
    "\n",
    "command = \"\"\"\n",
    "SET search_path = ag_catalog, \"$user\", public;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    conn.rollback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psycopg2\n",
    "import jinja2\n",
    "import json\n",
    "import tiktoken\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def get_distinct_trait(conn, trait):\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM cypher('movies_graph', $$\n",
    "        MATCH (t:{trait})\n",
    "        RETURN DISTINCT t.name as name\n",
    "    $$) as (name text);\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "    return [row[0] for row in results]\n",
    "\n",
    "def get_distinct_unprocessed_trait(conn, trait):\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM cypher('movies_graph', $$\n",
    "        MATCH (t:{trait})\n",
    "        WHERE NOT EXISTS(t.summary)\n",
    "        RETURN DISTINCT t.name as name\n",
    "    $$) as (name text);\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "    return [row[0] for row in results]\n",
    "\n",
    "def get_movies_by_trait(conn, trait, trait_name, edge_name):\n",
    "    safe_trait_name = trait_name.replace(\"'\", \"\\\\'\")\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM cypher('movies_graph', $$\n",
    "        MATCH (m:Movie)-[:{edge_name}]->(t:{trait} {{name: '{safe_trait_name}'}})\n",
    "        RETURN m.combined_text\n",
    "    $$) as (combined_text text);\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        rows = cursor.fetchall()\n",
    "    texts = [row[0] for row in rows if row[0]]\n",
    "    return texts\n",
    "\n",
    "def cap_tokens(input, max_tokens=128000, encoding='cl100k_base'):\n",
    "    tokenizer = tiktoken.get_encoding(encoding)\n",
    "    tokens = tokenizer.encode(input)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "    return tokenizer.decode(tokens)\n",
    "\n",
    "def get_summary(system_prompt_template, user_prompt_template, trait_name, combined_texts, llm_client, llm_model=\"gpt-4o\"):\n",
    "    system_prompt = system_prompt_template.render(name=trait_name)\n",
    "    user_prompt = user_prompt_template.render(combined_texts=combined_texts)\n",
    "    capped_user_prompt = cap_tokens(user_prompt, max_tokens=120000)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": capped_user_prompt}\n",
    "    ]\n",
    "    completion = llm_client.beta.chat.completions.parse(  \n",
    "        model=llm_model,\n",
    "        messages=messages,\n",
    "        max_tokens=4000,  \n",
    "        temperature=0.7,\n",
    "    )\n",
    "    response_content = completion.choices[0].message.content\n",
    "    return response_content\n",
    "\n",
    "def store_summary(conn, trait, trait_name, summary):\n",
    "    safe_summary = summary.replace(\"'\", \"\\\\'\")\n",
    "    safe_trait_name = trait_name.replace(\"'\", \"\\\\'\")\n",
    "    query = f\"\"\"\n",
    "    SELECT * FROM cypher('movies_graph', $$\n",
    "        MATCH (t:{trait} {{name: '{safe_trait_name}'}})\n",
    "        SET t.summary = '{safe_summary}'\n",
    "        RETURN t\n",
    "    $$) as (t agtype);\n",
    "    \"\"\"\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(query)\n",
    "        result = cursor.fetchall()\n",
    "    conn.commit()\n",
    "\n",
    "def process_single_trait(trait, edge_name, trait_name, system_prompt_template, user_prompt_template, llm_client, db_conn, llm_model):\n",
    "    combined_texts = get_movies_by_trait(conn=db_conn, trait=trait, trait_name=trait_name, edge_name=edge_name)\n",
    "    summary = get_summary(system_prompt_template, user_prompt_template, trait_name, combined_texts, llm_client, llm_model)\n",
    "    store_summary(conn=db_conn, trait=trait, trait_name=trait_name, summary=summary)\n",
    "    return trait_name\n",
    "\n",
    "def process_trait_parallel(conn, trait, edge_name, system_prompt_template_path, user_prompt_template_path, llm_client, llm_model=\"gpt-4o\"):\n",
    "    with open(system_prompt_template_path, 'r') as f:\n",
    "        system_prompt_template = jinja2.Template(f.read())\n",
    "    with open(user_prompt_template_path, 'r') as f:\n",
    "        user_prompt_template = jinja2.Template(f.read())\n",
    "    traits = get_distinct_unprocessed_trait(conn, trait)\n",
    "    total = len(traits)\n",
    "    print(f\"Processing {total} unprocessed {trait}\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=400) as executor:\n",
    "        futures = {executor.submit(process_single_trait, trait, edge_name, trait_name, system_prompt_template, user_prompt_template, llm_client, conn, llm_model): trait_name for trait_name in traits}\n",
    "        for idx, future in enumerate(as_completed(futures)):\n",
    "            trait_name = futures[future]\n",
    "            try:\n",
    "                _ = future.result()  # Processed trait summary stored\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {trait_name}: {e}\")\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(f\"Processed {idx + 1} out of {total} of {trait}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize traits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genre\n",
    "process_trait_parallel(conn=conn, trait=\"Genre\", edge_name=\"IN_GENRE\", system_prompt_template_path=\"prompts/summarize_genre.jinja2\", user_prompt_template_path=\"prompts/summarize_user.jinja2\", llm_client=gpt_4o_client, llm_model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series\n",
    "process_trait_parallel(conn=conn, trait=\"Series\", edge_name=\"PART_OF_SERIES\", system_prompt_template_path=\"prompts/summarize_series.jinja2\", user_prompt_template_path=\"prompts/summarize_user.jinja2\", llm_client=gpt_4o_mini_client, llm_model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character\n",
    "process_trait_parallel(conn=conn, trait=\"Character\", edge_name=\"FEATURES_CHARACTER\", system_prompt_template_path=\"prompts/summarize_character.jinja2\", user_prompt_template_path=\"prompts/summarize_user.jinja2\", llm_client=gpt_4o_mini_client, llm_model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theme\n",
    "process_trait_parallel(conn=conn, trait=\"Theme\", edge_name=\"INCLUDES_THEME\", system_prompt_template_path=\"prompts/summarize_theme.jinja2\", user_prompt_template_path=\"prompts/summarize_user.jinja2\", llm_client=gpt_4o_mini_client, llm_model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting\n",
    "process_trait_parallel(conn=conn, trait=\"Setting\", edge_name=\"SET_IN\", system_prompt_template_path=\"prompts/summarize_setting.jinja2\", user_prompt_template_path=\"prompts/summarize_user.jinja2\", llm_client=gpt_4o_mini_client, llm_model=\"gpt-4o-mini\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
