{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG demo level 2\n",
    "In more advanced demonstration we will add hierarchical and graph approaches by extracting metadata, finding and storing relationships between documents and adding summarizations for aggregate questions.\n",
    "\n",
    "## Step 4 - Creating and storing embeddings (vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using postgresql://psqladmin:)ycxlsxlLRKks*g#@psql-graphrag-psbv.postgres.database.azure.com/demo?sslmode=require as the database connection string\n",
      "Using https://graphrag-psbv.openai.azure.com/ as the embedding model endpoint\n",
      "Using https://graphrag-psbv.openai.azure.com/ as the gpt-4o-mini model endpoint\n",
      "Using https://graphrag-psbv.openai.azure.com/ as the gpt-4o model endpoint\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "\n",
    "original_dir = os.getcwd()\n",
    "try:\n",
    "    # Jump into the terraform directory\n",
    "    os.chdir('terraform')\n",
    "\n",
    "    # Get the database connection string\n",
    "    PGHOST = subprocess.run(['terraform', 'output', '-raw', 'PGHOST'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    PGDATABASE = subprocess.run(['terraform', 'output', '-raw', 'PGDATABASE'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    PGUSER = subprocess.run(['terraform', 'output', '-raw', 'PGUSER'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    PGPASSWORD = subprocess.run(['terraform', 'output', '-raw', 'PGPASSWORD'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    db_uri = f\"postgresql://{PGUSER}:{PGPASSWORD}@{PGHOST}/{PGDATABASE}?sslmode=require\"\n",
    "\n",
    "    # Get the embedding model endpoint and key\n",
    "    model_configurations = subprocess.run(['terraform', 'output', '-raw', 'model_configurations'], stdout=subprocess.PIPE).stdout.decode('utf-8')\n",
    "    model_config = json.loads(model_configurations)\n",
    "    embedding_model = model_config[\"models\"][\"text-embedding-3-large\"]\n",
    "    EMBEDDINGS_ENDPOINT = embedding_model[\"endpoint\"]\n",
    "    EMBEDDINGS_KEY = embedding_model[\"key\"]\n",
    "    gpt_4o_mini_model = model_config[\"models\"][\"gpt-4o-mini\"]\n",
    "    GPT_4O_MINI_ENDPOINT = gpt_4o_mini_model[\"endpoint\"]\n",
    "    GPT_4O_MINI_KEY = gpt_4o_mini_model[\"key\"]\n",
    "    gpt_4o_model = model_config[\"models\"][\"gpt-4o\"]\n",
    "    GPT_4O_ENDPOINT = gpt_4o_model[\"endpoint\"]\n",
    "    GPT_4O_KEY = gpt_4o_model[\"key\"]\n",
    "\n",
    "    print(f\"Using {db_uri} as the database connection string\")\n",
    "    print(f\"Using {EMBEDDINGS_ENDPOINT} as the embedding model endpoint\")\n",
    "    print(f\"Using {GPT_4O_MINI_ENDPOINT} as the gpt-4o-mini model endpoint\")\n",
    "    print(f\"Using {GPT_4O_ENDPOINT} as the gpt-4o model endpoint\")\n",
    "\n",
    "finally:\n",
    "    os.chdir(original_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "from openai import AzureOpenAI \n",
    "import pandas as pd\n",
    "import age\n",
    "\n",
    "conn = psycopg2.connect(db_uri)\n",
    "\n",
    "gpt_embedding_client = AzureOpenAI(\n",
    "    azure_endpoint=EMBEDDINGS_ENDPOINT,\n",
    "    api_key=EMBEDDINGS_KEY,\n",
    "    api_version=\"2025-02-01-preview\",\n",
    ")\n",
    "\n",
    "# Enable AGE for this connection\n",
    "command = \"\"\"\n",
    "SET search_path = ag_catalog, \"$user\", public;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with conn.cursor() as cursor:\n",
    "        cursor.execute(command)\n",
    "        conn.commit()\n",
    "except psycopg2.Error as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    conn.rollback()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to create node type standard tables with vectors and to process texts in all node types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def create_table(conn, name):\n",
    "    command = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS public.{name} (\n",
    "        id BIGINT,\n",
    "        embedding vector(2000)\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command)\n",
    "            conn.commit()\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        conn.rollback()\n",
    "\n",
    "def get_unprocessed_nodes(conn, node_name, text_column_name):\n",
    "    command = f\"\"\"\n",
    "    SELECT * FROM cypher('movies_graph', $$\n",
    "        MATCH (n:{node_name})\n",
    "        WHERE exists(n.{text_column_name}) AND n.{text_column_name} <> ''\n",
    "        RETURN n.{text_column_name} AS content, ID(n) AS id\n",
    "    $$) as (content text, id bigint)\n",
    "    WHERE id NOT IN (SELECT id FROM public.{node_name});\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(command)\n",
    "            rows = cursor.fetchall()\n",
    "            return rows\n",
    "    except psycopg2.Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return []\n",
    "\n",
    "def create_embeddings(conn, node_name, text_column_name):\n",
    "    # Create the table if it doesn't exist\n",
    "    create_table(conn, node_name)\n",
    "\n",
    "    # Get unprocessed nodes\n",
    "    data_to_process = get_unprocessed_nodes(conn, node_name, text_column_name)\n",
    "    if not data_to_process:\n",
    "        print(f\"No unprocessed nodes found for {node_name}.\")\n",
    "        return\n",
    "    else:\n",
    "        print(f\"Found {len(data_to_process)} unprocessed nodes for {node_name}.\")\n",
    "    \n",
    "    # Process the data in batches\n",
    "    batch_size = 100\n",
    "    max_retries = 10\n",
    "    retry_delay = 10\n",
    "    processed = 0\n",
    "    total = len(data_to_process)\n",
    "    print(f\"Processing {total} nodes in batches of {batch_size}.\")\n",
    "\n",
    "    for i in range(0, len(data_to_process), batch_size):\n",
    "        batch = data_to_process[i:i + batch_size]\n",
    "        texts = [row[0] for row in batch]\n",
    "        ids = [row[1] for row in batch]\n",
    "\n",
    "        # Create embeddings using the Azure OpenAI client with retry on 429 errors\n",
    "        for attempt in range(1, max_retries + 1):\n",
    "            try:\n",
    "                result = gpt_embedding_client.embeddings.create(\n",
    "                    input=texts,\n",
    "                    model=\"text-embedding-3-large\",\n",
    "                    dimensions=2000,\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if hasattr(e, \"status_code\") and e.status_code == 429:\n",
    "                    print(f\"429 error encountered. Retry attempt {attempt} of {max_retries} in {retry_delay} seconds...\")\n",
    "                    time.sleep(retry_delay)\n",
    "                else:\n",
    "                    print(f\"Embedding generation error: {e}\")\n",
    "                    raise e\n",
    "        else:\n",
    "            print(\"Failed to generate embeddings after multiple retries.\")\n",
    "            return\n",
    "\n",
    "        embeddings = [item.embedding for item in result.data]\n",
    "\n",
    "        # Insert the embeddings into the database\n",
    "        insert_command = f\"\"\"\n",
    "        INSERT INTO public.{node_name} (id, embedding) VALUES (%s, %s)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.executemany(insert_command, list(zip(ids, embeddings)))\n",
    "                conn.commit()\n",
    "        except psycopg2.Error as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            conn.rollback()\n",
    "        processed += len(ids)\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {processed}/{total} nodes.\")\n",
    "    print(f\"Finished processing {total} nodes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No unprocessed nodes found for Movie.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No unprocessed nodes found for Setting.\n",
      "No unprocessed nodes found for Character.\n",
      "No unprocessed nodes found for Genre.\n",
      "No unprocessed nodes found for Theme.\n",
      "No unprocessed nodes found for Series.\n"
     ]
    }
   ],
   "source": [
    "nodes = [\n",
    "    {\n",
    "        \"name\": \"Movie\",\n",
    "        \"text_column_name\": \"combined_text\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Setting\",\n",
    "        \"text_column_name\": \"summary\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Character\",\n",
    "        \"text_column_name\": \"summary\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Genre\",\n",
    "        \"text_column_name\": \"summary\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Theme\",\n",
    "        \"text_column_name\": \"summary\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Series\",\n",
    "        \"text_column_name\": \"summary\",\n",
    "    },\n",
    "]\n",
    "for node in nodes:\n",
    "    create_embeddings(conn=conn, node_name=node[\"name\"], text_column_name=node[\"text_column_name\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
