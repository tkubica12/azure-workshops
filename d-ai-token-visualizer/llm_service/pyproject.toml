[project]
name = "token-visualizer-llm-service"
version = "0.1.0"
description = "Local LLM Service for Token Visualizer - FastAPI service providing Gemma 2 model inference"
readme = "README.md"
requires-python = ">=3.12"
dependencies = [
    "fastapi>=0.104.0",
    "uvicorn[standard]>=0.24.0",
    "pydantic>=2.5.0",
    "pydantic-settings>=2.0.0",
    "transformers>=4.40.0",
    "accelerate>=0.28.0",
    "bitsandbytes>=0.42.0",
    "python-dotenv>=1.0.0",
    "huggingface_hub>=0.20.0",
    "httpx>=0.28.1",
]

[project.optional-dependencies]
cpu = [
    "torch>=2.0.0",
    "torchvision>=0.15.0",
    "torchaudio>=2.0.0",
]
gpu-cuda128 = [
    "torch>=2.0.0",
    "torchvision>=0.15.0", 
    "torchaudio>=2.0.0",
]

[tool.uv]
conflicts = [
    [
        { extra = "cpu" },
        { extra = "gpu-cuda128" },
    ],
]

[tool.uv.sources]
torch = [
    { index = "pytorch-cpu", extra = "cpu" },
    { index = "pytorch-cuda128", extra = "gpu-cuda128" },
]
torchvision = [
    { index = "pytorch-cpu", extra = "cpu" },
    { index = "pytorch-cuda128", extra = "gpu-cuda128" },
]
torchaudio = [
    { index = "pytorch-cpu", extra = "cpu" },
    { index = "pytorch-cuda128", extra = "gpu-cuda128" },
]

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-cuda128"
url = "https://download.pytorch.org/whl/cu128"
explicit = true

