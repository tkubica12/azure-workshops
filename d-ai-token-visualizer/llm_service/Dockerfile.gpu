# Dockerfile for Local LLM Service (GPU version)
FROM nvidia/cuda:12.8-runtime-ubuntu22.04

# Set working directory
WORKDIR /app

# Install Python, system dependencies and uv
RUN apt-get update && apt-get install -y \
    python3 \
    python3-pip \
    python3-venv \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/* \
    && curl -LsSf https://astral.sh/uv/install.sh | sh

# Add uv to PATH and create python symlink
ENV PATH="/root/.cargo/bin:$PATH"
RUN ln -s /usr/bin/python3 /usr/bin/python

# Copy project files
COPY pyproject.toml uv.lock* ./

# Install Python dependencies using uv (GPU version)
RUN uv sync --extra gpu-cuda128 --frozen --no-dev

# Copy application code
COPY . .

# Create non-root user
RUN useradd --create-home --shell /bin/bash appuser && \
    chown -R appuser:appuser /app
USER appuser

# Expose port
EXPOSE 8001

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:8001/health || exit 1

# Run the application using uv
CMD ["uv", "run", "main.py"]
