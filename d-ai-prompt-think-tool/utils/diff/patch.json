{
  "changes": [
    {
      "StartPositionMatch": "## Basics\n\nThis section covers the basic concepts and elements of GPT prompts.\n\nText prompts are how users interact with GPT models. As with all generative language models, GPT models attempt to produce the next series of words that are most likely to follow from the previous text. It's as if we're saying *What is the first thing that comes to your mind when I say `<prompt>`?* The examples below demonstrate this behavior. Given the first words of famous content, the model is able to accurately continue the text.\n\n| Prompt                             |Completion |",
      "EndPositionMatch": "### Prompt components\n\nWhen using the Completion API while there's no differentiation between different parts of the prompt, it can still be useful for learning and discussion to identify underlying prompt components. With the [Chat Completion API](../how-to/chatgpt.md) there are distinct sections of the prompt that are sent to the API in the form of an array of dictionaries with associated roles: system, user, and assistant. This guidance focuses more generally on how to think about prompt construction rather than providing prescriptive guidance that is specific to one API over another.\n",
      "ChangedText": "\n\n*__Basics of Prompt Engineering__*\n\n"
    },
    {
      "StartPositionMatch": "## Scenario-specific guidance\n\nWhile the principles of prompt engineering can be generalized across many different model types, certain models expect a specialized prompt structure. For Azure OpenAI GPT models, there are currently two distinct APIs where prompt engineering comes into play:\n",
      "EndPositionMatch": "#### [Chat completion APIs](#tab/chat)\n\n[!INCLUDE [Prompt Chat Completion](../includes/prompt-chat-completion.md)]\n",
      "ChangedText": "\n\n*__Guidance for Specific Scenarios__*\n\n"
    },
    {
      "StartPositionMatch": "## Best practices\n\n- **Be Specific**. Leave as little to interpretation as possible. Restrict the operational space.\n- **Be Descriptive**. Use analogies.\n- **Double Down**. Sometimes you might need to repeat yourself to the model. Give instructions before and after your primary content, use an instruction and a cue, etc.\n- **Order Matters**. The order in which you present information to the model might impact the output. Whether you put instructions before your content (“summarize the following…”) or after (“summarize the above…”) can make a difference in output. Even the order of few-shot examples can matter. This is referred to as recency bias.\n- **Give the model an “out”**. It can sometimes be helpful to give the model an alternative path if it is unable to complete the assigned task. For example, when asking a question over a piece of text you might include something like \"respond with \"not found\" if the answer is not present.\" This can help the model avoid generating false responses.\n\n## Space efficiency\n",
      "EndPositionMatch": "## Related content\n\n* [Learn more about Azure OpenAI](../overview.md).\n* Get started with the ChatGPT model with [the ChatGPT quickstart](../chatgpt-quickstart.md).\n* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples)\n",
      "ChangedText": "\n\n*__Best Practices for Prompt Engineering__*\n\n"
    },
    {
      "StartPositionMatch": "## Space efficiency\n\nWhile the input size increases with each new generation of GPT models, there will continue to be scenarios that provide more data than the model can handle. GPT models break words into \"tokens.\" While common multi-syllable words are often a single token, less common words are broken in syllables. Tokens can sometimes be counter-intuitive, as shown by the example below which demonstrates token boundaries for different date formats. In this case, spelling out the entire month is more space efficient than a fully numeric date. The current range of token support goes from 2,000 tokens with earlier GPT-3 models to up to 32,768 tokens with the 32k version of the latest GPT-4 model.\n",
      "EndPositionMatch": "## Related content\n\n* [Learn more about Azure OpenAI](../overview.md).\n* Get started with the ChatGPT model with [the ChatGPT quickstart](../chatgpt-quickstart.md).\n* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples)\n",
      "ChangedText": "\n\n*__Optimizing Space Efficiency__*\n\n"
    },
    {
      "StartPositionMatch": "## Related content\n\n* [Learn more about Azure OpenAI](../overview.md).\n* Get started with the ChatGPT model with [the ChatGPT quickstart](../chatgpt-quickstart.md).\n* For more examples, check out the [Azure OpenAI Samples GitHub repository](https://github.com/Azure/openai-samples)\n\n---\ntitle: Planning red teaming for large language models (LLMs) and their applications\ntitleSuffix: Azure OpenAI Service\ndescription: Learn about how red teaming and adversarial testing are an essential practice in the responsible development of systems and features using large language models (LLMs)\nms.service: azure-ai-openai\nms.topic: conceptual\nms.date: 03/27/2025\nmanager: nitinme\nauthor: mrbullwinkle\nms.author: mbullwin\nrecommendations: false\n---\n\n# Planning red teaming for large language models (LLMs) and their applications\n\nThis guide offers some potential strategies for planning how to set up and manage red teaming for responsible AI (RAI) risks throughout the large language model (LLM) product life cycle.\n\n## What is red teaming?\n",
      "EndPositionMatch": "## Why is RAI red teaming an important practice?\n\nRed teaming is a best practice in the responsible development of systems and features using LLMs. While not a replacement for systematic measurement and mitigation work, red teamers help to uncover and identify harms and, in turn, enable measurement strategies to validate the effectiveness of mitigations.\n",
      "ChangedText": "\n\n*__Related Resources__*\n\n"
    },
    {
      "StartPositionMatch": "## What is red teaming?\n\nThe term *red teaming* has historically described systematic adversarial attacks for testing security vulnerabilities. With the rise of LLMs, the term has extended beyond traditional cybersecurity and evolved in common usage to describe many kinds of probing, testing, and attacking of AI systems. With LLMs, both benign and adversarial usage can produce potentially harmful outputs, which can take many forms, including harmful content such as hate speech, incitement or glorification of violence, or sexual content.\n\n## Why is RAI red teaming an important practice?\n",
      "EndPositionMatch": "## Before testing\n\n### Plan: Who will do the testing\n",
      "ChangedText": "\n\n*__Definition of Red Teaming__*\n\n"
    },
    {
      "StartPositionMatch": "## Why is RAI red teaming an important practice?\n\nRed teaming is a best practice in the responsible development of systems and features using LLMs. While not a replacement for systematic measurement and mitigation work, red teamers help to uncover and identify harms and, in turn, enable measurement strategies to validate the effectiveness of mitigations.\n\nWhile Microsoft has conducted red teaming exercises and implemented safety systems (including [content filters](./content-filter.md) and other [mitigation strategies](./prompt-engineering.md)) for its Azure OpenAI Service models (see this [Overview of responsible AI practices](/legal/cognitive-services/openai/overview)), the context of each LLM application will be unique and you also should conduct red teaming to:\n",
      "EndPositionMatch": "## Before testing\n\n### Plan: Who will do the testing\n",
      "ChangedText": "\n\n*__Importance of RAI Red Teaming__*\n\n"
    },
    {
      "StartPositionMatch": "## Before testing\n\n### Plan: Who will do the testing\n\n**Assemble a diverse group of red teamers**\n",
      "EndPositionMatch": "### Plan: What to test\n\nBecause an application is developed using a base model, you might need to test at several different layers:\n",
      "ChangedText": "\n\n*__Preparation Before Testing__*\n\n"
    },
    {
      "StartPositionMatch": "### Plan: What to test\n\nBecause an application is developed using a base model, you might need to test at several different layers:\n",
      "EndPositionMatch": "### Plan: How to test\n\n**Conduct open-ended testing to uncover a wide range of harms.**\n",
      "ChangedText": "\n\n*__What to Test__*\n\n"
    },
    {
      "StartPositionMatch": "### Plan: How to test\n\n**Conduct open-ended testing to uncover a wide range of harms.**\n",
      "EndPositionMatch": "### Plan: How to record data\n\n**Decide what data you need to collect and what data is optional.**\n",
      "ChangedText": "\n\n*__How to Test__*\n\n"
    },
    {
      "StartPositionMatch": "### Plan: How to record data\n\n**Decide what data you need to collect and what data is optional.**\n",
      "EndPositionMatch": "## During testing\n\n**Plan to be on active standby while red teaming is ongoing**\n",
      "ChangedText": "\n\n*__How to Record Data__*\n\n"
    },
    {
      "StartPositionMatch": "## During testing\n\n**Plan to be on active standby while red teaming is ongoing**\n\n- Be prepared to assist red teamers with instructions and access issues.\n- Monitor progress on the spreadsheet and send timely reminders to red teamers.\n\n## After each round of testing\n",
      "EndPositionMatch": "## After each round of testing\n\n**Report data**\n",
      "ChangedText": "\n\n*__During Testing__*\n\n"
    },
    {
      "StartPositionMatch": "## After each round of testing\n\n**Report data**\n\nShare a short report on a regular interval with key stakeholders that:\n",
      "EndPositionMatch": "The guidance in this document is not intended to be, and should not be construed as providing, legal advice. The jurisdiction in which you're operating may have various regulatory or legal requirements that apply to your AI system. Be aware that not all of these recommendations are appropriate for every scenario and, conversely, these recommendations may be insufficient for some scenarios.\n</document>",
      "ChangedText": "\n\n*__After Each Round of Testing__*\n\n"
    }
  ]
}